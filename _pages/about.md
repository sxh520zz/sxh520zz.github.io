---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Iâ€™m a third-year Ph.D. student in the [School of Informatics](https://www.i.nagoya-u.ac.jp/) at [Nagoya University](https://www.nagoya-u.ac.jp/). I am supervised by [Prof. Tomoki Toda](https://sites.google.com/site/tomokitoda/home) from the Information Technology Center at Nagoya University. 

Prior to this, I was supervised by [Prof. Jianwu Dang](https://scholar.google.com/citations?user=Wk5ApskAAAAJ&hl=zh-CN&oi=ao) from the School of Information Science at the Japan Advanced Institute of Science and Technology. From October 2020 to April 2022, I was an algorithm researcher at the AI Research Institute, Hithink RoyalFlush, working on speech emotion recognition and synthesis under the supervision of [Prof. Xinhui Hu](https://scholar.google.com/citations?user=WhCsrgoAAAAJ&hl=zh-CN&oi=ao).

My research interests include emotion recognition, emotion synthesis, emotion conversion, and dialogue systems. 
<!-- I have published more than 10 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->

# ğŸ“– Educations
- *2022.10 - 2025.10 (Now)*, [Nagoya University](https://www.nagoya-u.ac.jp/), Aichi, Japan. ğŸ‡¯ğŸ‡µ
- *2019.04 - 2020.10*,  [Japan Advanced Institute of Science and Technology](https://www.jaist.ac.jp/index.html), Ishikawa, Japan. ğŸ‡¯ğŸ‡µ
- *2013.09 - 2017.06*, [Shaanxi University of Technology](https://www.snut.edu.cn/), Shaanxi, China. ğŸ‡¨ğŸ‡³

# ğŸ’» Careers
- *2020.10 - 2022.04*, AI Research Institute, Hithink RoyalFlush, Hangzhou, China. 

# ğŸ”¥ News
- *2025.05*: &nbsp;ğŸ‰ğŸ‰ Three first-author papers accepted at INTERSPEECH 2025 (Rotterdam, Netherlands). ğŸ‡³ğŸ‡±
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ First author paper and two co-authored papers accepted at APSIPA ASC 2024 (Macau, China). ğŸ‡²ğŸ‡´
- *2024.06*: MNSæ¬¡ä¸–ä»£ç ”ç©¶äº‹æ¥­ 2024å¹´åº¦ é«˜è²¢çŒ® RESEARDENT.
- *2024.06*: &nbsp;ğŸ‰ğŸ‰ First author paper accepted at INTERSPEECH 2024 (Kos, Greek). ğŸ‡¬ğŸ‡·
- *2023.12*: &nbsp;ğŸ‰ğŸ‰ Co-First author paper accepted at ICASSP 2024 (Seoul, South Korea). ğŸ‡°ğŸ‡·
- *2023.07*: &nbsp;ğŸ‰ğŸ‰ Co-authored paper accepted at MRAC 2023 (Ottawa, Canada). ğŸ‡¨ğŸ‡¦
- *2023.06*: &nbsp;ğŸ‰ğŸ‰ Co-authored paper accepted to IEEE Transactions ASLP.
- *2023.05*: &nbsp;ğŸ‰ğŸ‰ First author paper accepted at INTERSPEECH 2023 (Dublin, Ireland). ğŸ‡®ğŸ‡ª
- *2022.09*: Started my Ph.D. degree at Nagoya University (Aichi, Japan). ğŸ‡¯ğŸ‡µ
- *2020.10*: Joined the AI Research Institute at Hithink RoyalFlush (Hangzhou, China). ğŸ‡¨ğŸ‡³
- *2020.05*: &nbsp;ğŸ‰ğŸ‰ First author paper accepted at INTERSPEECH 2020 (Shanghai, China). ğŸ‡¨ğŸ‡³
- *2018.04*: Started my masterâ€™s degree at Japan Advanced Institute of Science and Technology (Ishikawa, Japan). ğŸ‡¯ğŸ‡µ

# ğŸ“ Publications 
*Journal:*
- <span style="display: inline; background-color: #e6f7ff; padding: 3px;"><code>IEEE Transactions ASLP</code></span> Xingfeng Li, **Xiaohan Shi**, Desheng Hu, Yongwei Li, Qingchen Zhang, Zhengxia Wang, Masashi Unoki, Masato Akagi. "**Music Theory-inspired Acoustic Representation for Speech Emotion Recognition.**" *IEEE/ACM Transactions on Audio, Speech and Language Processing* Vol. 31, pp. 2534-2547, June 2023.

- **Xiaohan Shi**, Jiajun He, Xingfeng Li, Tomoki Toda. "**On the effectiveness of ASR representations in real-world noisy speech emotion recognition.**" Submitted to *IEEE/ACM Transactions on Audio, Speech and Language Processing*.

- **Xiaohan Shi**, Xingfeng Li, Tomoki Toda. "**Emotion Similarity and Shift: Modeling Temporal Dynamic Interactions for Emotion Prediction in Conversation.**" Submitted to *IEEE/ACM Transactions on Audio, Speech and Language Processing*.

- Jiajun He, **Xiaohan Shi**, Cheng-Hung Hu, Jinyi Mi, Xingfeng Li, Tomoki Toda. "**M4SER: Multimodal Multi-Representation Multi-Task Multi-Strategy Learning for Speech Emotion Recognition.**" Submitted to *IEEE/ACM Transactions on Audio, Speech and Language Processing*.

- Xingfeng Li, **Xiaohan Shi**, Masashi Unoki, Tomoki Toda, Masato Akagi. "**The Contribution of Perceptual Semantic Primitives to Self-Supervised Learning Representations for Bilingual Speech Emotion Recognition.**" Submitted to *Speech Communication*.

- Yongwei Li, **Xiaohan Shi**, Xingfeng Li, Yang Liu, Zhen Zhao, Jianhua Tao, Aijun Li, Donna Erickson, Tomoki Toda, Masto Akagi, Feng Du. "**Comparative Analysis of Handcrafted Acoustic Features and Pre-Trained Embeddings for Robust Speech Emotion Recognition Under Varying Noise Conditions.**" Submitted to *IEEE Transactions on Affective Computing*.

- Xingfeng Li, Feifei Yu, **Xiaohan Shi**, Tian Xu, Kai Li, Ningfeng Luo, Yang Liu. "**Phase-Aware Spectrogram Fusion with Dual-Stream Residual Networks for Underwater Acoustic Recognition.**" Submitted to *IEEE/ACM Transactions on Audio, Speech and Language Processing*.

- Xingfeng Li, Junjie Li, **Xiaohan Shi**, Yuke Si, Kai Li, Tian Xu. "**Valence-Arousal Emotion Recognition Using A Deep Three-Layer Model with Aural Perceptual Representations.**" Submitted to *IEEE/ACM Transactions on Audio, Speech and Language Processing*.

- Xingfeng Li, Jiahao Kang, Tian Xu, **Xiaohan Shi**, Kai Li, Yang Liu. "**Revisiting Pitch: From Music to Speech, Using Colored Pitch Analysis to Enhance Emotion and Speaker Recognition.**" Submitted to *Knowledge-Based Systems*.

- Xingfeng Li, Ningfeng Luo, Feifei Yu, Junjie Li, Kai Li, Yongwei Li, Zhen Zhao, Yang Liu, **Xiaohan Shi**. "**HARL: Human Auditory Representation Learning for Cross-Dialect Bird Species Recognition.**" Submitted to *Engineering Applications of Artificial Intelligence*.



*Conference:*
- <span style="display: inline; background-color: #fce4ec; padding: 3px;"><code>INTERSPEECH</code></span> **Xiaohan Shi**, Sixia Li, Jianwu Dang. "**Dimensional Emotion Prediction Based on Interactive Context in Conversation.**" *In Proc. INTERSPEECH*, pp.4193-4197, 2020.

- <span style="display: inline; background-color: #fce4ec; padding: 3px;"><code>INTERSPEECH</code></span> **Xiaohan Shi**, Xingfeng Li, Tomoki Toda. "**Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation.**" *In Proc. INTERSPEECH*, pp.765-769, 2023.

- <span style="display: inline; background-color: #fce4ec; padding: 3px;"><code>INTERSPEECH</code></span> **Xiaohan Shi**, Xingfeng Li, Tomoki Toda. "**Multimodal Fusion of Music Theory-Inspired and Self-Supervised Representations for Improved Emotion Recognition.**" *In Proc. INTERSPEECH*, pp.4193-4197, 2024.

- <span style="display: inline; background-color: #fce4ec; padding: 3px;"><code>INTERSPEECH</code></span> **Xiaohan Shi**, Xingfeng Li, Tomoki Toda. "**Who, When, and What: Leveraging the "Three Ws" Concept for Emotion Recognition in Conversation.**" *In Proc. INTERSPEECH*, 2025.

- <span style="display: inline; background-color: #fce4ec; padding: 3px;"><code>INTERSPEECH</code></span> **Xiaohan Shi**, Xingfeng Li, Tomoki Toda. "**Speaker-Aware Multi-Task Learning for Speech Emotion Recognition.**" *In Proc. INTERSPEECH*, 2025.
 
- <span style="display: inline; background-color: #fce4ec; padding: 3px;"><code>INTERSPEECH</code></span>  **Xiaohan Shi**, Jinyi Mi, Xingfeng Li, Tomoki Toda. "**Advancing Emotion Recognition via Ensemble Learning: Integrating Speech, Context, and Text Representations.**" *In Proc. INTERSPEECH*, 2025.

- <span style="display: inline; background-color: #f2f2f2; padding: 3px;"><code>APSIPA</code></span> **Xiaohan Shi**, Yuan Gao, Jiajun He, Jinyi Mi, Xingfeng Li, Tomoki Toda. "**A Study on Multimodal Fusion and Layer Adaptor in Emotion Recognition.**" *Proc. APSIPA ASC*, 2024.

- <span style="display: inline; background-color: #fffde7; padding: 3px;"><code>ICASSP</code></span> Jiajun He\*, **Xiaohan Shi\***, Xingfeng Li, Tomoki Toda. "**Mf-Aed-Aec: Speech Emotion Recognition by Leveraging Multimodal Fusion, Asr Error Detection, And Asr Error Correction.**" *In Proc. IEEE-ICASSP*, pp.11066-11070, 2024
  
- <span style="display: inline; background-color: #f2f2f2; padding: 3px;"><code>APSIPA</code></span> Jinyi Mi, **Xiaohan Shi**, Takuya Fujimura, Tomoki Toda. "**Two-stage Framework for Robust Speech Emotion Recognition Using Target Speaker Extraction in Human Speech Noise Conditions.**" *Proc. APSIPA ASC*, 2024.

- <span style="display: inline; background-color: #f2f2f2; padding: 3px;"><code>APSIPA</code></span> Xingfeng Li, **Xiaohan Shi**, Yuke Si, Qian Chen, Yang Liu, Masashi Unoki, Masato Akagi. "**BEES: A New Acoustic Task for Blended Emotion Estimation in Speech.**" *Proc. APSIPA ASC*, 2024.
  
- <span style="display: inline; background-color: #e8f5e9; padding: 3px;"><code>MRAC</code></span> Jingguang Tian\*, Desheng Hu\*, **Xiaohan Shi**, Jiajun He, Xingfeng Li, Yuan Gao, Tomoki Toda, Xinkang Xu, Xinhui Hu. "**Semi-supervised Multimodal Emotion Recognition with Consensus Decision-making and Label Correction.**" *In Proc. MRAC*, pp.67â€“73, 2023.

- Kai Yang, Zijian Bai, Yang Xiao, Xinyu Li, **Xiaohan Shi**. "**RGB-Phase Speckle: Cross-Scene Stereo 3D Reconstruction via Wrapped Pre-Normalization.**" Submitted to *ICCV 2025*.

# ğŸ Competitions
- **MER 2023: Multi-label Learning, Semi-Supervised Learning, and Modality Robustness.**  
   Track 1: **7**/23;  Track 2: **8**/23;  Track 3: **4**/15.

- **Odyssey 2024: Speech Emotion Recognition Challenge.**  
   Track 1: **11**/17;  Track 2: **5**/15.

- **Interspeech 2025: Speech Emotion Recognition in Naturalistic Conditions Challenge.**  
   Track 1: **6**/43.

# ğŸ– Honors and Awards
*Fellowship:*
- *2019.10 - 2020.10*, çŸ³å·çœŒç§è²»å¤–å›½äººç•™å­¦ç”Ÿå¥¨å­¦é‡‘. 
- *2022.10 - 2025.10*, æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹èåˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢æ¬¡ä¸–ä»£ç ”ç©¶äº‹æ¥­(èåˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢æ¬¡ä¸–ä»£ãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼).
- *2024.06 - 2025.03*, MNSæ¬¡ä¸–ä»£ç ”ç©¶äº‹æ¥­ 2024å¹´åº¦ é«˜è²¢çŒ® RESEARDENT.
  
*Reviewer:*
- *2023.06 - (Now)*, Speech Communication.
- *2023.07*, Asia-Pacific Signal and Information Processing Association (APSIPA ASC).
- *2023.07*, IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).

# ğŸ’¬ Visitor
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=600&t=tt&d=jEuxYdVJyECxniDOS1wDEtmil2J7WoZ8HaUtzPOBOcU&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>


<!-- 
# ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.
 -->

---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Iâ€™m a third-year Ph.D. student in the [School of Informatics](https://www.i.nagoya-u.ac.jp/) at [Nagoya University](https://www.nagoya-u.ac.jp/). I am supervised by Prof. [Tomoki Toda](https://sites.google.com/site/tomokitoda/home) from the School of Informatics at Nagoya University. 

Prior to this, I was supervised by [Prof. Jianwu Dang](https://scholar.google.com/citations?user=Wk5ApskAAAAJ&hl=zh-CN&oi=ao) from the School of Information Science at the Japan Advanced Institute of Science and Technology.

From October 2020 to April 2022, I was an algorithm researcher at the AI Research Institute, Hithink RoyalFlush, working on speech emotion recognition and synthesis under the supervision of [Prof. Xinhui Hu](https://scholar.google.com/citations?user=WhCsrgoAAAAJ&hl=zh-CN&oi=ao).

My research interests include emotion recognition, synthesis, conversion, and dialogue systems. 
<!-- I have published more than 10 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->

# ğŸ“– Educations
- *2022.10 - 2025.10 (Now)*, [Nagoya University](https://www.nagoya-u.ac.jp/), Aichi, Japan. 
- *2019.04 - 2020.10*,  [Japan Advanced Institute of Science and Technology](https://www.jaist.ac.jp/index.html), Ishikawa, Japan. 
- *2013.09 - 2017.06*, [Shaanxi University of Technology](https://www.snut.edu.cn/), Shaanxi, China.

# ğŸ’» Working Experiences
- *2020.10 - 2022.04*, AI Research Institute, Hithink RoyalFlush, Hangzhou, China. 

# ğŸ”¥ News
- *2025.01*: MNSæ¬¡ä¸–ä»£ç ”ç©¶äº‹æ¥­ 2025å¹´åº¦ é«˜è²¢çŒ® RESEARDENT.. 
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ First author paper and two co-authored [paper,paper] accepted at APSIPA ASC 2024 (Macau, China).
- *2024.06*: MNSæ¬¡ä¸–ä»£ç ”ç©¶äº‹æ¥­ 2024å¹´åº¦ é«˜è²¢çŒ® RESEARDENT.
- *2024.06*: &nbsp;ğŸ‰ğŸ‰ First author paper accepted at INTERSPEECH 2024 (Kos, Greek).
- *2023.12*: &nbsp;ğŸ‰ğŸ‰ Co-First author paper accepted at ICASSP 2024 (Seoul, South Korea).
- *2023.07*: &nbsp;ğŸ‰ğŸ‰ Co-authored paper accepted at MRAC 2023 (Ottawa, Canada).
- *2023.06*: &nbsp;ğŸ‰ğŸ‰ Co-authored paper accepted to IEEE Transactions ASLP.
- *2023.05*: &nbsp;ğŸ‰ğŸ‰ First author paper accepted at INTERSPEECH 2023 (Dublin, Ireland).
- *2022.09*: Started my Ph.D. degree at Nagoya University (Aichi, Japan).
- *2020.10*: Joined the AI Research Institute at Hithink RoyalFlush (Hangzhou, China).
- *2020.05*: &nbsp;ğŸ‰ğŸ‰ First author paper accepted at INTERSPEECH 2020 (Shanghai, China).
- *2018.04*: Started my masterâ€™s degree at Japan Advanced Institute of Science and Technology (Ishikawa, Japan).

# ğŸ“ Publications 
*Journal:*
- <code class="language-plaintext highlighter-rouge">IEEE Transactions ASLP</code> Xingfeng Li, **Xiaohan Shi**, Desheng Hu, Yongwei Li, Qingchen Zhang, Zhengxia Wang, Masashi Unoki, Masato Akagi. "**Music Theory-inspired Acoustic Representation for Speech Emotion Recognition.**" *IEEE/ACM Transactions on Audio, Speech and Language Processing* Vol. 31, pp. 2534-2547, June 2023.

*Conference:*
- <code class="language-plaintext highlighter-rouge">INTERSPEECH</code> **Xiaohan Shi**, Sixia Li, Jianwu Dang. "**Dimensional Emotion Prediction Based on Interactive Context in Conversation.**" *In Proc. INTERSPEECH*, pp.4193-4197, 2020.
  
- <code class="language-plaintext highlighter-rouge">INTERSPEECH</code> **Xiaohan Shi**, Xingfeng Li, Tomoki Toda. "**Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation.**" *In Proc. INTERSPEECH*, pp.765-769, 2023.

- <code class="language-plaintext highlighter-rouge">INTERSPEECH</code> **Xiaohan Shi**, Xingfeng Li, Tomoki Toda. "**Multimodal Fusion of Music Theory-Inspired and Self-Supervised Representations for Improved Emotion Recognition.**" *In Proc. INTERSPEECH*, pp.4193-4197, 2024.

- <code class="language-plaintext highlighter-rouge">APSIPA</code> **Xiaohan Shi**, Yuan Gao, Jiajun He, Jinyi Mi, Xingfeng Li, Tomoki Toda. "**A Study on Multimodal Fusion and Layer Adaptor in Emotion Recognition.**" *Proc. APSIPA ASC*, 2024.

- <code class="language-plaintext highlighter-rouge">ICASSP</code> Jiajun He\*, **Xiaohan Shi\***, Xingfeng Li, Tomoki Toda. "**Mf-Aed-Aec: Speech Emotion Recognition by Leveraging Multimodal Fusion, Asr Error Detection, And Asr Error Correction.**" *In Proc. IEEE-ICASSP*, pp.11066-11070, 2024

- <code class="language-plaintext highlighter-rouge">APSIPA</code> Jinyi Mi, **Xiaohan Shi**, Takuya Fujimura, Tomoki Toda. "**Two-stage Framework for Robust Speech Emotion Recognition Using Target Speaker Extraction in Human Speech Noise Conditions.**" *Proc. APSIPA ASC*, 2024.

- <code class="language-plaintext highlighter-rouge">APSIPA</code> Xingfeng Li, **Xiaohan Shi**, Yuke Si, Qian Chen, Yang Liu, Masashi Unoki, Masato Akagi. "**BEES: A New Acoustic Task for Blended Emotion Estimation in Speech.**" *Proc. APSIPA ASC*, 2024.

- <code class="language-plaintext highlighter-rouge">MRAC</code> Jingguang Tian\*, Desheng Hu\*, **Xiaohan Shi**, Jiajun He, Xingfeng Li, Yuan Gao, Tomoki Toda, Xinkang Xu, Xinhui Hu. "**Semi-supervised Multimodal Emotion Recognition with Consensus Decision-making and Label Correction.**" *In Proc. MRAC*, pp.67â€“73, 2023.
 
# ğŸ– Honors and Awards
*Fellowship:*
- *2019.10 - 2020.10* çŸ³å·çœŒç§è²»å¤–å›½äººç•™å­¦ç”Ÿå¥¨å­¦é‡‘. 
- *2022.10 - 2025.10* æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹èåˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢æ¬¡ä¸–ä»£ç ”ç©¶äº‹æ¥­ï¼ˆèåˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢æ¬¡ä¸–ä»£ãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ï¼‰.
- *2024.06 - 2025.03* MNSæ¬¡ä¸–ä»£ç ”ç©¶äº‹æ¥­ 2024å¹´åº¦ é«˜è²¢çŒ® RESEARDENT.
- *2025.03 - 2025.10* MNSæ¬¡ä¸–ä»£ç ”ç©¶äº‹æ¥­ 2025å¹´åº¦ é«˜è²¢çŒ® RESEARDENT.
  
*Reviewer:*
- *2023.06 - now* Speech Communication.
- *2023.07* Asia-Pacific Signal and Information Processing Association (APSIPA ASC).
- *2023.07* IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) (2023).
  
<!-- 
# ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.
 -->
